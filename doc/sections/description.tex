% !TEX root = ../main.tex

% Background section

\section{Variational Boosting}\label{sec:vb}
The variational boosting algorithm uses the mixture
\begin{align*}
q^{(C)}(x;\psi) = \sum_{c=1}^C \lambda_c q_c(x; \psi_c) \ \text{ s.t. } \ \lambda_c \geq 0 \ \text{ and } \ \sum_{c}\lambda_c = 1
\end{align*}
to approximate some (possibly unnormalized) target density $\pi$, where $\psi_c$ denotes the parameters that characterize each of the $C$ components from some pre-specified familty of distributions and $\lambda_c$'s are the corresponding mixture weights. The VB algorithm builds such mixture approximations by iteratively adding a new component to the existing approximation. Specifically, at iteration $t$, we find
\begin{align}
\psi_t^*, \lambda_t^* \in \arg\min_{\psi_t, \lambda_t} D_{KL} \left( \lambda_t q_t(\cdot;\psi_t) + (1-q_t)q^{(t-1)}(\cdot;\psi) \| \pi \right),
\label{eq:subproblem}
\end{align}
With $q^{t}(\cdot;\psi) = \lambda_t^* q_t(\cdot;\psi_t^*) + (1-q_t^*)q^{(t-1)}(\cdot;\psi)$ becoming the updated mixture approximation. If this subproblem of component optimization can be solved efficiently, we will have greatly exdended the single pre-specified variational family to its convex hull.\\\\
To build this mixture approximation, we can simply start by using the approximation from standard VI methods as the first component in the VB mixture approximation. Common choices include ADVI and BBVI \cite{ranganath2014black,kucukelbir2017automatic}. The component optimization problem as shown in \cref{eq:subproblem} may seem difficult to solve at the beginning. However, note that given a mixture approximation $q^{C}$, we can take advantage of the mixture structure and get
\begin{align}\label{eq:component.wise}
D_{KL} \left( q^{(C)} \| \pi \right) = \mathbb{E}_{q^{(C)}} \left[ \ln q^{(C)}(x;\psi,\lambda) - \ln\pi(x) \right] = \sum_{c=1}^C \lambda_c \mathbb{E}_{q_c} \left[ \ln q^{(C)}(x;\psi,\lambda) - \ln\pi(x) \right].
\end{align}
Note the expectation is now with respect to each mixture component. If the mixture components are Gaussian, we can then apply the reparameterization trick on each component \cite{kingma2013auto}. Then the gradient of the above expression can be estimated by sampling from a standard normal distribution and transform these samples based on each components mean and variance. This way the component optimization problem can be solved using stochastic gradient descent (SGD) methods. Since all of the existing components' means and variances stay fixed, at each iteration we only need to optimize the parameters of the component to be added along with its mixture weight. By repeating this process iteratively, we have not only increased the expressibility of our variational family but also created the desirable tradeoff between computation time and quality of approximation.\\\\
As hinted above, the Gaussian distribution family is one of the most commonly used in the context of variational boosting. However, as the dimension increases, the number of latent parameters to be optimized in the covariance matrix also increases. This can make the optimization of each component extremely difficult. \cite{miller2017variational} suggested imposing a ``low rank plus diagonal'' structure on the covariance matrix. Namely, we let $\Sigma = D + FF^T$, where $\Sigma, D \in \mathbb{R}^{d\times d}, D$ being diagonal, and $F \in \mathbb{R}^{d\times r}$, with $\text{rank}(F)=r$. This formulation can greatly reduce the number of parameters in the covariance matrix by setting $r \ll d$. A heuristic for selecting $r$ is also mentioned in \cite{miller2017variational}, whose intuition comes from approximating a single multivariate normal target. With some structure imposed on the covariance matrix, we are bound to underestimate the variance of the target distribution. Starting with $r=1$, as we increase the rank, the marginal variances of the variational approximation should also increase. Therefore, it is suggested that, when fitting the initial component in the VB framework, we can keep increasing the rank until the change in the marginal variances are negligible. All subsequent components then share this covariance structure picked by the first component. This intuition makes sense in practice. In high dimensional data, in many cases we have dimensions that are close to marginally independent. Then ignoring some of these covariance terms can simplify the optimization without greatly hurting the quality of the approximation.\\\\
Finally, \cite{miller2017variational} also addressed another common issue in the implementation of variational boosting that it is sensitive to the initialization of each component. As an example, when approximating some bimodal target distribution, we may get stuck in one of the modes that are already well-captured if the initialization of the next component falls under this mode. Therefore, when initializing the next component, we would like to place it in a region of the target that is currently under-approimated. The suggest approach is the weighted EM algorithm. Assuming we are still using Gaussian mixture component, at iteration $t$, we can first sample from the current approximation of $t-1$ components before finding $t$ clusters. With the first $t-1$ cluster's mean and variances fixed, we optimize the mean and variance, as well as the mixture weight of the $t$th cluster, which will be used as the initialization of the next component. By incorporating the weights, which is the ratio of the likelihoods between the target distribution and the current approximation, more importance will be placed on points that are in an under-approximated region of the target distribution.
% ...