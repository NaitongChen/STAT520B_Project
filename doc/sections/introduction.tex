% !TEX root = ../main.tex

% Background section

\section{Introduction}
In recent years, Bayesian statistical models have received a lot of attention thanks to their ability to model data of complex structures, incorporate domain expertise, and coherently quantify uncertainties around the estimates of the latent parameters. One of the key challenges of applying Bayesian statistical models in practice is to obtain samples from the often intractable posterior distribution. Markov chain Monte Carlo (MCMC) and Variational Inference (VI) are the two main approaches that address this challenge. MCMC generates posterior samples by simulating a Markov chain whose invariant distribution is the target posterior distribution. One of the main advantage of MCMC is that there is a tradeoff between computation time and the quality of the posterior samples: as more samples are generated by simulating the Markov chain for longer time, one can expect the samples to be better approximate the posterior distribution. However, MCMC does not scale well to problems with large datasets. Specifically, due to the iterative nature of MCMC, the log likelihood of each observation in the entire dataset needs to be evaluated before a single posterior sample can be generated. VI handles this scalability issue by approximating the target using some tractable distribution. More precisely, VI finds some distribution from a tractable distribution family that minimizes some divergence to the target distribution. The tractablity of the posterior approximation allows us to easily sample from the posterior distribution. However, the quality of the posterior approximation under the VI approach is fundamentally limited by the choice of the variational family. In other words, the approximation does not get closer to the true target distribution with more computation if the variational family is poorly chosen.\\\\
Inspired by the idea of boosting, the Variational Boosting (VB) algorithm proposed in \cite{miller2017variational} addressed this shortcoming of the VI methods. Specifically, VB builds a mixture approximation of the target by interatively adding components from some variational family. This approach not only increases the expressibility of a fixed variational family by extending the variational family to its convex hull, but also forms the tradeoff between computation time and the quality of approximation, just like MCMC. A later paper then confirmed that under certain conditions, as the number of components increased, the mixture approximation from VB would indeed converge to the true target distribution \cite{locatello2018boosting}.\\\\
We note that the work in \cite{miller2017variational} focused on a single variational family. Although it has been shown that the mixture approximation converges to the target distribution as the number of mixture components increase, the choice of variational family may still result in varying efficiencies of the algorithm. An example is to approximate a heavy-tailed target distribution using mixtures of Gaussian components. The light-tailedness of the Gaussian distribution may hinder the rate of convergence in terms of the number of components, requiring many mixture components to obtain reasonable approximations of the target distribution's tail behaviour. One possible way to address this limitation is to further diversify the variational family by considering multiple candidate components from different distribution families at each iteration. More concretely, at each iteration, instead of optimizing the next component to be added to the mixture approximation from a single distribution family, multiple candidate components from different distribution families can be optimized at the same time. Then the component that yields the mixture approximation with the smallest divergence to the target distribution is added.\\\\
In this report, we begin in \cref{sec:vb} by reviewing the VB algorithm and two practical considerations that improve the efficiency of the method introduced in \cite{miller2017variational}. Experiments conducted on synthetic data are shown in \cref{sec:verify} to verify and critique the effectiveness of the VB algorithm. We then describe our proposed extension and analyze some experiment results on some synthetic data in \cref{sec:extension,sec:experiments}.\\\\
The code used to run the experiments and generate the plots can be found in \url{https://github.com/NaitongChen/STAT520B_Project}.

% ...