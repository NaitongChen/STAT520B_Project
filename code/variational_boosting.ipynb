{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "female-desire",
   "metadata": {},
   "source": [
    "# Extending Variational Boosting to Multiple Competing Variational Families\n",
    "\n",
    "This jupyter notebook contains all of the code used to generate the plots in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "digital-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, ReverseDiff, ForwardDiff, SpecialFunctions, Distributions, Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-undergraduate",
   "metadata": {},
   "source": [
    "## ADVI\n",
    "\n",
    "We begin by implementing the ADVI algorithm that can be used to obtain the first component in the mixture approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "id": "secure-cache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADVI (generic function with 3 methods)"
      ]
     },
     "execution_count": 1305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, num_iter)\n",
    "    Σs = zeros(d, d, num_iter)\n",
    "    obj = zeros(num_iter)\n",
    "\n",
    "    for k=1:num_iter\n",
    "        # transform ψ to mean and covariance\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        # store parameter\n",
    "        μs[:,k] = μ\n",
    "        Σs[:,:,k] = L * transpose(L)\n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        # compute the gradient\n",
    "        ∇ψ = grad(y, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "        # take a step\n",
    "        ψ0 = ψ0 .- γ(k) .* ∇ψ\n",
    "        # estimate the objective function\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        xobj = rand(MvNormal(μ, L * transpose(L)), Nobj)\n",
    "        for i=1:size(xobj)[1]\n",
    "            obj[k] += logpdf(MvNormal(μ, L * transpose(L)), xobj[:,i]) - logπ(xobj[:,i])\n",
    "        end\n",
    "        obj[k] /= Nobj\n",
    "    end\n",
    "\n",
    "    return μs, Σs, obj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "id": "buried-colors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad (generic function with 1 method)"
      ]
     },
     "execution_count": 1306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transform(ψ, diag_sig, d)\n",
    "    μ = ψ[1:d]\n",
    "    if diag_sig\n",
    "        L = Diagonal(exp.(ψ[d+1:d+d]./2))\n",
    "    else\n",
    "        L = reshape(ψ[d+1:end], (d, d))\n",
    "    end\n",
    "    return μ, L\n",
    "end\n",
    "\n",
    "function logπψ(y, ψ0, logπ, diag_sig, d)\n",
    "    μ, L = transform(ψ0, diag_sig, d)\n",
    "    x = vec(μ .+ L * y)\n",
    "    return -(logπ(x) .+ log(abs(det(L))))\n",
    "end\n",
    "\n",
    "function grad(ys, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "    ret = zeros(size(ψ0)[1])\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])    \n",
    "        ret = ret .+ ForwardDiff.gradient(ψψ -> logπψ(y, ψψ, logπ, diag_sig, d), ψ0)\n",
    "    end\n",
    "    return (ret./Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-singer",
   "metadata": {},
   "source": [
    "## Variational Boosting\n",
    "\n",
    "We now implement the variational boosting algorithm. First we implement the weighted EM method for initializing new components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "id": "immediate-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_from_current_mixture (generic function with 2 methods)"
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function sample_from_current_mixture(μs, Σs, λs, Ngrad, d)\n",
    "    xs = zeros(d, Ngrad)\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:Ngrad\n",
    "        # identify component\n",
    "        u = rand(Uniform(0,1))\n",
    "        comp = 0\n",
    "        for j in 1:num_comp\n",
    "            if u <= sum(λs[1:j])\n",
    "                comp = j\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # sample from component\n",
    "        xs[:,i] = rand(MvNormal(vec(μs[:,comp]), Σs[:,:,comp]))\n",
    "    end\n",
    "    return xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "id": "variable-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mixture (generic function with 2 methods)"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function log_mixture(μs, Σs, λs, x)\n",
    "    mix = 0.\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:num_comp\n",
    "        mix = mix + λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    return log(mix)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "id": "together-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_EM_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function compute_EM_weights(μs, Σs, λs, logπ, xs)\n",
    "    N = size(xs)[2]\n",
    "    logws = zeros(N)\n",
    "    for i in 1:N\n",
    "        x = xs[:,i]\n",
    "        logws[i] = logπ(x) - log_mixture(μs, Σs, λs, x)\n",
    "    end\n",
    "    \n",
    "    println(exp.(logws))\n",
    "    return exp.(logws)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "id": "steady-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weighted_EM_initialization (generic function with 3 methods)"
      ]
     },
     "execution_count": 1310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function weighted_EM_initialization(μs, Σs, λs, weights, xs, diag_sig)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    N = size(xs)[2]\n",
    "    d = size(μs)[1]\n",
    "    λ = 1. / num_comp\n",
    "    μ = vec(zeros(d))\n",
    "    Σ = Diagonal(ones(d))\n",
    "    # EM loop\n",
    "    keep = true\n",
    "    while(keep)\n",
    "        # E step\n",
    "        resp = zeros(N, num_comp)\n",
    "        λs_curr = (1 - λ) .* λs\n",
    "        for i in 1:N\n",
    "            x = vec(xs[:,i])\n",
    "            for j in 1:num_comp\n",
    "                if j < num_comp\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μs[:,j]), Σs[:,:,j]), x) * λs_curr[j]\n",
    "                else\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μ),Σ), x) * λ\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        # normalize by dividing row sum\n",
    "        rowsum = sum(resp, dims=2)\n",
    "        resp = resp ./ rowsum\n",
    "        # M step\n",
    "        Nk = sum(resp, dims=1)[num_comp]\n",
    "        newλ = Nk / N\n",
    "        newμ = vec(zeros(d))\n",
    "        newΣ = zeros(d,d)\n",
    "        # update μ\n",
    "        for i in 1:N\n",
    "            newμ = newμ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* vec(xs[:,i])\n",
    "        end\n",
    "        # update Σ\n",
    "        for i in 1:N\n",
    "            diff = vec(xs[:,i]) .- newμ\n",
    "            newΣ = newΣ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* (diff) * transpose(diff)\n",
    "        end\n",
    "        # ensure Σ is Hermitian\n",
    "        for j in 1:d\n",
    "            for k in 1:j\n",
    "                newΣ[j,k] = newΣ[k,j]\n",
    "            end\n",
    "        end\n",
    "        # check convergence\n",
    "        if (newλ - λ)^2 <= 1e-3\n",
    "            println(\"finished initializing ψ and λ\")\n",
    "            keep = false\n",
    "        end\n",
    "        # update params\n",
    "        λ = newλ\n",
    "        μ = newμ\n",
    "        Σ = newΣ\n",
    "    end\n",
    "    # modify Σ to follow specification of diag_sig\n",
    "    if diag_sig\n",
    "        Σ = Diagonal(Σ)\n",
    "    end\n",
    "\n",
    "    return λ, μ, Σ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-maldives",
   "metadata": {},
   "source": [
    "The following functions compute the gradient for component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "id": "floral-strap",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mix (generic function with 2 methods)"
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "    ret = 0.\n",
    "    for i in 1:size(μs)[2]\n",
    "        ret = ret + (1 - λ) * λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    ret = ret + λ * pdf(MvNormal(vec(μ_opt), L_opt * transpose(L_opt)), vec(x))\n",
    "    return log(ret)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "satisfactory-actor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_comp (generic function with 2 methods)"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function obj_comp(y, ψ, λ, logπ, diag_sig, d, μs, Σs, λs)\n",
    "    μ_opt, L_opt = transform(ψ, diag_sig, d)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    ret = 0.\n",
    "    for i in 1:num_comp\n",
    "        if i < num_comp\n",
    "            if diag_sig\n",
    "                L = Diagonal(sqrt.(Σs[:,:,i]))\n",
    "            else\n",
    "                C = cholesky(Σs[:,:,i])\n",
    "                L = convert(Array{Float64,2}, C.L)\n",
    "            end\n",
    "            x = vec(μs[:,i] .+ L * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - (1-λ) * λs[i] * logπ(x)\n",
    "            ret = ret + (1-λ) * λs[i] * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        else\n",
    "            x = vec(μ_opt .+ L_opt * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - λ * logπ(x)\n",
    "            ret = ret + λ * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "id": "available-moment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_comp (generic function with 2 methods)"
      ]
     },
     "execution_count": 1313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function grad_comp(ys, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    retψ = zeros(size(ψ)[1])\n",
    "    retλ = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])\n",
    "        retψ = retψ .+ ForwardDiff.gradient(ψψ -> obj_comp(y, ψψ, λ, logπ, diag_sig, d, μs, Σs, λs), ψ)\n",
    "        retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logπ, diag_sig, d, μs, Σs, λs), λ)\n",
    "    end\n",
    "    return (retψ./Ngrad), (retλ/Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-knife",
   "metadata": {},
   "source": [
    "The following function performs the component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "creative-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize_component (generic function with 2 methods)"
      ]
     },
     "execution_count": 1314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize_component(μs, Σs, λs, λ_k, μ_k, Σ_k, logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "    # turn μ_k and Σ_k to the format of ψ\n",
    "    ψ0 = copy(μ_k)\n",
    "    if diag_sig\n",
    "        append!(ψ0, log.([Σ_k[i,i] for i in 1:d]))\n",
    "    else\n",
    "        C = cholesky(Σ_k)\n",
    "        append!(ψ0, vec(C.L))\n",
    "    end\n",
    "    \n",
    "    ψ = vec(copy(ψ0))\n",
    "    println(ψ)\n",
    "    println(typeof(ψ))\n",
    "\n",
    "    # start optimization\n",
    "    λks = zeros(2*num_iter)\n",
    "    μks = zeros(d, num_iter)\n",
    "    Σks = zeros(d, d, num_iter)\n",
    "    λ = λ_k\n",
    "    \n",
    "    for k=1:(2*num_iter)\n",
    "        if k <= num_iter\n",
    "            # transform ψ to mean and covariance\n",
    "            μ, L = transform(ψ, diag_sig, d)\n",
    "            # store parameter\n",
    "            μks[:,k] = μ\n",
    "            Σks[:,:,k] = L * transpose(L)\n",
    "        end\n",
    "\n",
    "        λks[k] = λ\n",
    "        \n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        \n",
    "        # compute the gradient\n",
    "        ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "        # take a step\n",
    "        if k <= num_iter\n",
    "            ψ = ψ .- γ(k) .* ∇ψ\n",
    "        end\n",
    "        λ = λ - (γ(k)/200) * ∇λ\n",
    "        # project λ to feasible region\n",
    "        if λ < 0.\n",
    "            λ = 0.\n",
    "        elseif λ > 1.\n",
    "            λ = 1.\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"-----\")\n",
    "    y = rand(MvNormal(d, 1), Ngrad)\n",
    "    ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    println(\"ψ gradient: \", ∇ψ)\n",
    "    println(\"λ gradient: \", ∇λ)\n",
    "    println(\"-----\")\n",
    "    \n",
    "    return λks[2*num_iter], μks[:,num_iter], Σks[:,:,num_iter]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-occasions",
   "metadata": {},
   "source": [
    "The following function performs variational boosting, where the first component is optimized using ADVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "id": "wrapped-telescope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 3 methods)"
      ]
     },
     "execution_count": 1315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "\n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        xs = sample_from_current_mixture(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], 40, d)\n",
    "        weights = compute_EM_weights(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], logπ, xs)\n",
    "        λ_k, μ_k, Σ_k = weighted_EM_initialization(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], weights, xs, diag_sig)\n",
    "        println(\"initialization of weight, mean, and variance\")\n",
    "        println(λ_k)\n",
    "        println(μ_k)\n",
    "        println(Σ_k)\n",
    "        println(\"-----\")\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-deputy",
   "metadata": {},
   "source": [
    "We first demonstrate a case where the weighted EM algorithm fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "id": "honey-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "mean: [1.9707038202545093, 1.9694340881629466]\n",
      "variance: [1.0992736232511053 0.0; 0.0 1.1026546848108385]\n",
      "-----\n",
      "optimizing component 2 / 2\n",
      "[0.5327189451215595, 0.5017136131829105, 0.540461719305902, 0.48381084318745093, 0.5521877769390933, 0.5207485482723334, 0.5486813198544986, 0.5062282961260438, 0.41576286968420956, 0.49790262773233823, 0.47172198807627275, 0.5375937775272542, 0.4951387501887072, 0.5178332480708243, 0.5223591431477888, 0.5186392865899537, 0.5510055950817563, 0.49205109423039295, 0.4940926564614897, 0.47845911540806024, 0.5236116175830913, 0.4346887122483684, 0.4993523407260072, 0.4837488994332491, 0.5479484389430723, 0.4399662455292501, 0.5357114546778512, 0.5322096263902948, 0.5155967618334488, 0.4442263986538946, 0.394379519583017, 0.504200687695202, 0.5551649393906588, 0.5208367133116818, 0.5532140135104433, 0.5291721601183221, 0.486076318386388, 0.44590770269986657, 0.51257389028276, 0.4779403373593588]\n",
      "finished initializing ψ and λ\n",
      "initialization of weight, mean, and variance\n",
      "0.01457136058640468\n",
      "[0.3713336339259033, 0.3801856904582055]\n",
      "[0.1634726795059797 0.0; 0.0 0.13519870637750753]\n",
      "-----\n",
      "[0.3713336339259033, 0.3801856904582055, -1.8111094004299797, -2.0010096836327507]\n",
      "Array{Float64,1}\n",
      "-----\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] try_yieldto(::typeof(Base.ensure_rescheduled), ::Base.RefValue{Task}) at .\\task.jl:611",
      " [2] wait() at .\\task.jl:668",
      " [3] uv_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at .\\stream.jl:935",
      " [4] unsafe_write(::Base.PipeEndpoint, ::Ptr{UInt8}, ::UInt64) at .\\stream.jl:1007",
      " [5] unsafe_write at .\\io.jl:593 [inlined]",
      " [6] unsafe_write(::Base.PipeEndpoint, ::Base.RefValue{UInt8}, ::Int64) at .\\io.jl:591",
      " [7] write at .\\io.jl:594 [inlined]",
      " [8] write(::Base.PipeEndpoint, ::UInt8) at .\\stream.jl:1045",
      " [9] write at .\\io.jl:309 [inlined]",
      " [10] write at .\\io.jl:647 [inlined]",
      " [11] print at .\\char.jl:229 [inlined]",
      " [12] print(::IJulia.IJuliaStdio{Base.PipeEndpoint}, ::String, ::Char) at .\\strings\\io.jl:46",
      " [13] println(::IJulia.IJuliaStdio{Base.PipeEndpoint}, ::String) at .\\strings\\io.jl:73",
      " [14] println(::String) at .\\coreio.jl:4",
      " [15] optimize_component(::Array{Float64,2}, ::Array{Float64,3}, ::Array{Float64,1}, ::Float64, ::Array{Float64,1}, ::Diagonal{Float64,Array{Float64,1}}, ::Function, ::var\"#3145#3146\", ::Int64, ::Int64, ::Int64, ::typeof(transform), ::Bool) at .\\In[1314]:50",
      " [16] VB(::Function, ::Function, ::Function, ::Int64, ::Int64, ::Int64, ::Array{Float64,1}, ::Int64, ::Int64, ::typeof(transform), ::Bool) at .\\In[1315]:31",
      " [17] top-level scope at In[1316]:14"
     ]
    }
   ],
   "source": [
    "μ = vec([2. 2.])\n",
    "Σ = [1 0.; 0. 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "id": "upper-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [1.9707038202545093, 1.9694340881629466]\n",
      "sigma: [1.0992736232511053 0.0; 0.0 1.1026546848108385]\n",
      "lambda: 0.9983550729078866\n",
      "mu: [0.36434653135468625, 0.37154584859953504]\n",
      "sigma: [0.16489604097628668 0.0; 0.0 0.13619115843002563]\n",
      "lambda: 0.0016449270921134356\n"
     ]
    }
   ],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "id": "southern-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -5:0.1:5\n",
    "x2s = -5:0.1:5\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "id": "technological-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-fashion",
   "metadata": {},
   "source": [
    "We hence discard the weighted EM initialization and instead intialize the next component with a component centred at the origin with a large variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "id": "hidden-genius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 3 methods)"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "    \n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        λ_k = 1. / Ncomp\n",
    "        Σ_k = Diagonal(ones(d))\n",
    "        idx = Int64(floor(rand()*k))+1\n",
    "        μ_k = -vec(ones(d))\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "id": "stylish-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "mean: [2.9941526171455397, 2.9928674816328695]\n",
      "variance: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "-----\n",
      "optimizing component 2 / 2\n",
      "[-1.0, -1.0, 0.0, 0.0]\n",
      "Array{Float64,1}\n",
      "-----\n",
      "ψ gradient: [0.06557776016618942, 0.05759889421271219, 0.06281868025584292, 0.002957233010158445]\n",
      "λ gradient: -0.059942449696490326\n",
      "-----\n",
      "component 2\n",
      "mean: [-2.85627062883778, -2.8546278764693818]\n",
      "variance: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "weight: 0.4808757132944286\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "μ = vec([3. 3.])\n",
    "Σ = [1 0.1; 0.1 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "id": "following-frederick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [2.9941526171455397, 2.9928674816328695]\n",
      "sigma: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "lambda: 0.5191242867055714\n",
      "mu: [-2.85627062883778, -2.8546278764693818]\n",
      "sigma: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "lambda: 0.4808757132944286\n"
     ]
    }
   ],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "eastern-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -6:0.1:6\n",
    "x2s = -6:0.1:6\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "closing-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_close\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-leisure",
   "metadata": {},
   "source": [
    "## Extending to Multiple Variational Families\n",
    "\n",
    "For this extension we focus on approximating mixture of Beta posteriors using log-normal and exponential components. We first implement the transformations so that the support of the variational family matches that of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "other-chapel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unconstrain (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes t in (0,∞) and constrains it to be in (0,1)\n",
    "function constrain(t)\n",
    "    x = 1/(t+1)\n",
    "    logJ = -2 * log(t+1)\n",
    "    return x, logJ\n",
    "end\n",
    "\n",
    "function unconstrain(x)\n",
    "    t = (1. / x) - 1\n",
    "    logJ = -2 * log(x)\n",
    "    return t, logJ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-davis",
   "metadata": {},
   "source": [
    "We now generalize the implementation of ADVI to both families of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "intelligent-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADVI (generic function with 1 method)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ADVI(logπ, grad, γ, Ngrad, ψ0, num_iter, constrain, κ, family)\n",
    "    # build the log density ρ that has unconstrained support\n",
    "    transform_logπ = (x, logJ) -> logπ(x) + logJ\n",
    "    logρ = x -> transform_logπ(constrain(x)...)\n",
    "    # store parameter values\n",
    "    if family == 1\n",
    "        ψs = zeros(2, num_iter)\n",
    "    elseif family == 2\n",
    "        ψs = zeros(num_iter)\n",
    "    end\n",
    "    #main loop\n",
    "    for k=1:num_iter\n",
    "        # store parameters\n",
    "        if family == 1\n",
    "            ψs[:,k] = ψ0\n",
    "        elseif family == 2\n",
    "            ψs[k] = ψ0\n",
    "        end\n",
    "        # sample from κ\n",
    "        ys = κ(Ngrad)\n",
    "        # compute the gradient\n",
    "        ∇ψ = grad(ys, ψ0, logρ, Ngrad)\n",
    "        # take a step\n",
    "        ψ0 = @. ψ0 - γ(k) * ∇ψ\n",
    "        if family == 1 && ψ0[2] < 0\n",
    "            ψ0[2] = 0.\n",
    "        elseif family == 2 && ψ0 < 0\n",
    "            ψ0 = 0.\n",
    "        end\n",
    "    end\n",
    "    return ψs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-formula",
   "metadata": {},
   "source": [
    "We now implement the functions that are specific to each variational family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "czech-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logq_exponential (generic function with 1 method)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function κ(Nobj)\n",
    "    return rand(Uniform(0,1), Nobj)\n",
    "end\n",
    "\n",
    "## log-normal\n",
    "\n",
    "function logρψ_gaussian(y, ψ, logρ)\n",
    "    x = exp(ψ[1] + sqrt(2 * ψ[2]^2) * erfinv(2 * y - 1))\n",
    "    trans = yy -> exp(ψ[1] + sqrt(2 * ψ[2]^2) * erfinv(2 * yy - 1))\n",
    "    g = ForwardDiff.derivative(yy -> trans(yy), y)\n",
    "    \n",
    "    return -(logρ(x) + log(abs(g)))\n",
    "end\n",
    "\n",
    "function grad_gaussian(ys, ψ0, logρ, Ngrad)\n",
    "    ret = zeros(2)\n",
    "    for i in 1:Ngrad\n",
    "        y = ys[i]\n",
    "        ret = ret .+ ForwardDiff.gradient(ψψ -> logρψ_gaussian(y, ψψ, logρ), ψ0)\n",
    "    end\n",
    "    return (ret ./ Ngrad)\n",
    "end\n",
    "\n",
    "function sample_gaussian_q(N, ψ)\n",
    "    y = rand(Uniform(), N)\n",
    "    t = @. exp(ψ[1] + sqrt(2 * ψ[2]^2) * erfinv(2 * y - 1))\n",
    "    samps = zero(t)\n",
    "    for i=1:size(samps)[1]\n",
    "        samps[i] = constrain(t[i])[1]\n",
    "    end\n",
    "    return samps\n",
    "end\n",
    "\n",
    "function logq_gaussian(x, ψ)\n",
    "    t, logJ = unconstrain(x) \n",
    "    log_f = logpdf(LogNormal(ψ[1], ψ[2]), t)\n",
    "    return logJ + log_f\n",
    "end\n",
    "\n",
    "## exponential\n",
    "\n",
    "function logρψ_exponential(y, ψ, logρ)\n",
    "    x = -log(1 - y) / ψ\n",
    "    return -(logρ(x) + log(abs(ψ^-1 + 1/(1-y))))\n",
    "end\n",
    "\n",
    "function grad_exponential(ys, ψ0, logρ, Ngrad)\n",
    "    ret = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = ys[i]\n",
    "        ret = ret + ForwardDiff.derivative(ψψ -> logρψ_exponential(y, ψψ, logρ), ψ0)\n",
    "    end\n",
    "    return (ret / Ngrad)\n",
    "end\n",
    "\n",
    "function sample_exponential_q(N, ψ)\n",
    "    y = rand(Uniform(), N)\n",
    "    t = @. -log(1 - y) / ψ\n",
    "    samps = zero(t)\n",
    "    for i=1:size(samps)[1]\n",
    "        samps[i] = constrain(t[i])[1]\n",
    "    end\n",
    "    return samps\n",
    "end\n",
    "\n",
    "function logq_exponential(x, ψ)\n",
    "    t, logJ = unconstrain(x) \n",
    "    log_f = logpdf(Exponential(ψ), t)\n",
    "    return logJ + log_f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-peter",
   "metadata": {},
   "source": [
    "We check the performance of ADVI. Note even with the exponential distribution, the approximation's density decreases as the input gets close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "accepted-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10970529530300867\n"
     ]
    }
   ],
   "source": [
    "logπ = x -> logpdf(Beta(1,5), x)\n",
    "γ = k -> 0.1/sqrt(k)\n",
    "Nobj = 20\n",
    "ψ0 = 0.5\n",
    "num_iter = 1000\n",
    "family = 2\n",
    "\n",
    "ψs = ADVI(logπ, grad_exponential, γ, Ngrad, ψ0, num_iter, constrain, κ, family);\n",
    "\n",
    "println(ψs[num_iter])\n",
    "\n",
    "samples = sample_exponential_q(5000, ψs[num_iter])\n",
    "histogram(samples, normed=true,bins=50, alpha=0.3, lavel = \"approximation\")\n",
    "plot!(0:0.001:1, x -> pdf(Beta(1, 5), x), label = \"target\", linewidth=1.5)\n",
    "png(\"exp_single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "lesbian-opposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9731540152499398, 1.1769161604675944]\n"
     ]
    }
   ],
   "source": [
    "logπ = x -> logpdf(Beta(1,5), x)\n",
    "γ = k -> 0.1/sqrt(k)\n",
    "Nobj = 20\n",
    "ψ0 = vec([0. 1.])\n",
    "num_iter = 1000\n",
    "family = 1\n",
    "\n",
    "ψs = ADVI(logπ, grad_gaussian, γ, Ngrad, ψ0, num_iter, constrain, κ, family);\n",
    "\n",
    "println(ψs[:,num_iter])\n",
    "\n",
    "samples = sample_gaussian_q(5000, ψs[:,num_iter])\n",
    "# samples = sample_gaussian_q(5000, [0. 0.5])\n",
    "histogram(samples, normed=true,bins=50, alpha=0.3, label = \"approximation\")\n",
    "plot!(0:0.001:1, x -> pdf(Beta(1, 5), x), label = \"target\", linewidth=1.5)\n",
    "png(\"gauss_single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-aerospace",
   "metadata": {},
   "source": [
    "We now implement Variational Boosting that considers both Gaussian and exponential components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "western-helena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mix (generic function with 1 method)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "# families: 1 encodes gaussian and 2 encodes exponential\n",
    "function log_mix(x, ψs, λs, families, ψ_opt, λ, family_opt)\n",
    "#     println(ψs)\n",
    "    ret = 0.\n",
    "    for i in 1:size(families)[1]\n",
    "        if families[i] == 1\n",
    "#             println(ψs[i][2])\n",
    "            ret = ret + (1 - λ) * λs[i] * pdf(LogNormal(ψs[i][1], ψs[i][2]), x)\n",
    "        else\n",
    "            ret = ret + (1 - λ) * λs[i] * pdf(Exponential(ψs[i]), x)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if family_opt == 1\n",
    "#         println(ψ_opt[2])\n",
    "        ret = ret + λ * pdf(LogNormal(ψ_opt[1], ψ_opt[2]), x)\n",
    "    else\n",
    "        ret = ret + λ * pdf(Exponential(ψ_opt), x)\n",
    "    end\n",
    "    \n",
    "    return log(ret)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "mysterious-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function obj_comp(y, ψ_opt, λ, logρ, ψs, λs, families, family_opt)\n",
    "    num_comp = size(families)[1] + 1\n",
    "    ret = 0.\n",
    "    for i in 1:num_comp\n",
    "        if i < num_comp\n",
    "            if families[i] == 1\n",
    "                x = exp(ψs[i][1] + sqrt(2 * ψs[i][2]^2) * erfinv(2 * y - 1))\n",
    "            else\n",
    "                x = -log(1 - y) / ψs[i]\n",
    "            end\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - (1-λ) * λs[i] * logρ(x)\n",
    "            ret = ret + (1-λ) * λs[i] * log_mix(x, ψs, λs, families, ψ_opt, λ, family_opt)\n",
    "        else\n",
    "            if family_opt == 1\n",
    "                x = exp(ψ_opt[1] + sqrt(2 * ψ_opt[2]^2) * erfinv(2 * y - 1))\n",
    "            else\n",
    "                x = -log(1 - y) / ψ_opt\n",
    "            end\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - λ * logρ(x)\n",
    "            ret = ret + λ * log_mix(x, ψs, λs, families, ψ_opt, λ, family_opt)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "mounted-immune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function grad_comp(ys, ψ, λ, logρ, Ngrad, ψs, λs, families, family_opt)\n",
    "    if family_opt == 1\n",
    "        retψ = zeros(2)\n",
    "    else\n",
    "        retψ = 0.\n",
    "    end\n",
    "    retλ = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = ys[i]\n",
    "        if family_opt == 1\n",
    "#             println(ψ)\n",
    "            retψ = retψ .+ ForwardDiff.gradient(ψψ -> obj_comp(y, ψψ, λ, logρ, ψs, λs, families, family_opt), ψ)\n",
    "            retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logρ, ψs, λs, families, family_opt), λ)\n",
    "        else\n",
    "            retψ = retψ + ForwardDiff.derivative(ψψ -> obj_comp(y, ψψ, λ, logρ, ψs, λs, families, family_opt), ψ)\n",
    "            retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logρ, ψs, λs, families, family_opt), λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if family_opt == 1 \n",
    "        return (retψ./Ngrad), (retλ/Ngrad)\n",
    "    else\n",
    "        return (retψ/Ngrad), (retλ/Ngrad)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "manual-remainder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize_component (generic function with 1 method)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize_component(ψs, λs, λ_k, ψ_k, logρ, γ, Ngrad, num_iter, families, family_opt, κ)\n",
    "    # start optimization\n",
    "    λks = zeros(2*num_iter)\n",
    "    if family_opt == 1\n",
    "        ψks = zeros(2, num_iter)\n",
    "    else\n",
    "        ψks = zeros(num_iter)\n",
    "    end\n",
    "\n",
    "    ψ = ψ_k\n",
    "    λ = λ_k\n",
    "\n",
    "    for k=1:(2*num_iter)\n",
    "        if k <= num_iter\n",
    "            if family_opt == 1\n",
    "                ψks[:,k] = ψ\n",
    "            else\n",
    "                ψks[k] = ψ\n",
    "            end\n",
    "        end\n",
    "\n",
    "        λks[k] = λ\n",
    "        \n",
    "        # sample from κ\n",
    "        ys = κ(Ngrad)\n",
    "        \n",
    "        # compute the gradient        \n",
    "        ∇ψ, ∇λ = grad_comp(ys, ψ, λ, logρ, Ngrad, ψs, λs, families, family_opt)\n",
    "\n",
    "        # take a step\n",
    "        if k <= num_iter            \n",
    "            ψ = @. ψ - γ(k) * ∇ψ\n",
    "            if family_opt == 1 && ψ[2] <= 0\n",
    "                ψ[2] = 0.1\n",
    "            elseif family_opt == 2 && ψ <= 0\n",
    "                ψ = 0.1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        λ = λ - (γ(k)/200) * ∇λ\n",
    "        # project λ to feasible region\n",
    "        if λ < 0.\n",
    "            λ = 0.\n",
    "        elseif λ > 1.\n",
    "            λ = 1.\n",
    "        end\n",
    "    end\n",
    "\n",
    "#     println(\"-----\")\n",
    "#     ys = κ(Ngrad)\n",
    "#     ∇ψ, ∇λ = grad_comp(ys, ψ, λ, logρ, Ngrad, ψs, λs, families, family_opt)\n",
    "#     println(\"ψ gradient: \", ∇ψ)\n",
    "#     println(\"λ gradient: \", ∇λ)\n",
    "#     println(\"-----\")\n",
    "    \n",
    "    if family_opt == 1\n",
    "        return λks[2*num_iter], ψks[:,num_iter]\n",
    "    else\n",
    "        return λks[2*num_iter], ψks[num_iter]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "purple-meter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_density_from_current_mixture (generic function with 1 method)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_from_current_mixture(Nobj, ψ, λ, family, ψs, λs, families)\n",
    "    xs = zeros(Nobj)\n",
    "    num_comp = size(families)[1] + 1\n",
    "    \n",
    "    λs = (1. - λ) .* λs\n",
    "    push!(λs, λ)\n",
    "    \n",
    "    for i in 1:Nobj\n",
    "        # identify component\n",
    "        u = rand(Uniform(0,1))\n",
    "        comp = 0\n",
    "        for j in 1:num_comp\n",
    "            if u <= sum(λs[1:j])\n",
    "                comp = j\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # sample from component\n",
    "        if comp < num_comp\n",
    "            if families[comp] == 1\n",
    "                xs[i] = sample_gaussian_q(1, ψs[comp])[1]\n",
    "            else\n",
    "                xs[i] = sample_exponential_q(1, ψs[comp])[1]\n",
    "            end\n",
    "        else\n",
    "            if family == 1\n",
    "                xs[i] = sample_gaussian_q(1, ψ)[1]\n",
    "            else\n",
    "                xs[i] = sample_exponential_q(1, ψ)[1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return xs\n",
    "end\n",
    "\n",
    "function log_density_from_current_mixture(x, ψ, λ, family, ψs, λs, families)\n",
    "    ret = 0.\n",
    "    for i in 1:size(families)[1]\n",
    "        if families[i] == 1\n",
    "            ret = ret + (1. - λ) * λs[i] * exp(logq_gaussian(x, ψs[i]))\n",
    "        else\n",
    "            ret = ret + (1. - λ) * λs[i] * exp(logq_exponential(x, ψs[i]))\n",
    "        end\n",
    "    end\n",
    "    if family == 1\n",
    "        ret = ret + λ * exp(logq_gaussian(x, ψ))\n",
    "    else\n",
    "        ret = ret + λ * exp(logq_exponential(x, ψ))\n",
    "    end\n",
    "    \n",
    "    return ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "practical-secretariat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "est_obj (generic function with 1 method)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function est_obj(Nobj, ψ, λ, family, logπ, ψs, λs, families, single)\n",
    "    if single\n",
    "        if family == 1\n",
    "            xobj = sample_gaussian_q(Nobj, ψ)\n",
    "        else\n",
    "            xobj = sample_exponential_q(Nobj, ψ)\n",
    "        end\n",
    "\n",
    "        ret = 0.\n",
    "        for i=1:Nobj\n",
    "            if family == 1\n",
    "                ret += logq_gaussian(xobj[i], ψ) - logπ(xobj[i])\n",
    "            else\n",
    "                ret += logq_exponential(xobj[i], ψ) - logπ(xobj[i])\n",
    "            end\n",
    "        end\n",
    "        ret /= Nobj\n",
    "\n",
    "        return ret\n",
    "    else\n",
    "        xobj = sample_from_current_mixture(Nobj, ψ, λ, family, ψs, λs, families)\n",
    "        ret = 0.\n",
    "        for i=1:Nobj\n",
    "            ret += log_density_from_current_mixture(xobj[i], ψ, λ, family, ψs, λs, families) - logπ(xobj[i])\n",
    "        end\n",
    "        ret /= Nobj\n",
    "        \n",
    "        return ret\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "brief-sweden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_from_current_mixture (generic function with 2 methods)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_from_current_mixture(Nobj, ψs, λs, families)\n",
    "    xs = zeros(Nobj)\n",
    "    num_comp = size(families)[1]\n",
    "\n",
    "    for i in 1:Nobj\n",
    "        # identify component\n",
    "        u = rand(Uniform(0,1))\n",
    "        comp = 0\n",
    "        for j in 1:num_comp\n",
    "            if u <= sum(λs[1:j])\n",
    "                comp = j\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # sample from component\n",
    "        if families[comp] == 1\n",
    "            xs[i] = sample_gaussian_q(1, ψs[comp])[1]\n",
    "        else\n",
    "            xs[i] = sample_exponential_q(1, ψs[comp])[1]\n",
    "        end\n",
    "    end\n",
    "    return xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "sorted-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 1 method)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, γ, Ncomp, Ngrad, Nobj, num_iter, \n",
    "            grad_gaussian, grad_exponential, κ, constrain)\n",
    "    transform_logπ = (x, logJ) -> logπ(x) + logJ\n",
    "    logρ = x -> transform_logπ(constrain(x)...)\n",
    "    \n",
    "    ψs = Any[]\n",
    "    families = zeros(Int8, Ncomp)\n",
    "    λs = [1.]\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    ψs_gaussian = ADVI(logπ, grad_gaussian, γ, Ngrad, vec([0. 1.]), num_iter, constrain, κ, 1)\n",
    "    ψs_exponential = ADVI(logπ, grad_exponential, γ, Ngrad, 0.5, num_iter, constrain, κ, 2)\n",
    "\n",
    "    # compare objectives\n",
    "    obj_gaussian = est_obj(Nobj, ψs_gaussian[:,num_iter], 0., 1, logπ, ψs, λs, families, true)\n",
    "    obj_exponential = est_obj(Nobj, ψs_exponential[num_iter], 0., 2, logπ, ψs, λs, families, true)\n",
    "    if obj_gaussian <= obj_exponential\n",
    "        push!(ψs, ψs_gaussian[:,num_iter])\n",
    "        families[1] = 1\n",
    "    else\n",
    "        push!(ψs, ψs_exponential[num_iter])\n",
    "        families[1] = 2\n",
    "    end\n",
    "\n",
    "    println(\"component 1\")\n",
    "    println(families)\n",
    "    println(ψs)\n",
    "    \n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        λ_k = 1. / k\n",
    "        # optimize new component\n",
    "        λ_k_gau, ψ_k_gau = optimize_component(ψs, λs, λ_k, vec([0. 0.5]), logρ, γ, Ngrad, num_iter, families[1:k-1], 1, κ)\n",
    "        λ_k_exp, ψ_k_exp = optimize_component(ψs, λs, λ_k, 7., logρ, γ, Ngrad, num_iter, families[1:k-1], 2, κ)\n",
    "        # compare objectives\n",
    "        obj_gaussian = est_obj(Nobj, ψ_k_gau, λ_k_gau, 1, logπ, ψs, λs, families[1:k-1], false)\n",
    "        obj_exponential = est_obj(Nobj, ψ_k_exp, λ_k_exp, 2, logπ, ψs, λs, families[1:k-1], false)\n",
    "        println(\"gau: \", obj_gaussian)\n",
    "        println(\"exp: \", obj_exponential)\n",
    "        # update parameters\n",
    "        if obj_gaussian <= obj_exponential\n",
    "            λs = (1. - λ_k_gau) .* λs\n",
    "            push!(λs, λ_k_gau)\n",
    "            push!(ψs, ψ_k_gau)\n",
    "            families[k] = 1\n",
    "        else\n",
    "            λs = (1. - λ_k_exp) .* λs\n",
    "            push!(λs, λ_k_exp)\n",
    "            push!(ψs, ψ_k_exp)\n",
    "            families[k] = 2\n",
    "        end\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"param: \", ψs[k])\n",
    "        println(\"weight: \", λs[k])\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return ψs, λs, families\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "boxed-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "Int8[2, 0]\n",
      "Any[0.23418498018439665]\n",
      "optimizing component 2 / 2\n",
      "gau: 1.2609888931655608\n",
      "exp: 4.645309650350763\n",
      "component 2\n",
      "param: [-1.115552618763077, 0.2636297188544139]\n",
      "weight: 0.28588861740644755\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "logπ = x -> log(0.5 * pdf(Beta(1,6), x) + 0.5 * pdf(Beta(30,10), x))\n",
    "γ = k -> 0.02 /sqrt(k)\n",
    "Ngrad = 100\n",
    "Nobj = 100\n",
    "num_iter = 3000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "ψs, λs, families = VB(logπ, γ, Ncomp, Ngrad, Nobj, num_iter, \n",
    "                        grad_gaussian, grad_exponential, κ, constrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "victorian-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_from_current_mixture(5000, ψs, λs, families)\n",
    "histogram(samples, normed=true,bins=50, alpha=0.3, label = \"approximation\")\n",
    "plot!(0:0.01:1, x -> exp(logπ(x)), label = \"target\", linewidth=1.5)\n",
    "png(\"both_multiple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "peaceful-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB_gaussian_only (generic function with 1 method)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB_gaussian_only(logπ, γ, Ncomp, Ngrad, Nobj, num_iter, \n",
    "                          grad_gaussian, grad_exponential, κ, constrain)\n",
    "    transform_logπ = (x, logJ) -> logπ(x) + logJ\n",
    "    logρ = x -> transform_logπ(constrain(x)...)\n",
    "    \n",
    "    ψs = Any[]\n",
    "    families = zeros(Int8, Ncomp)\n",
    "    λs = [1.]\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    ψs_gaussian = ADVI(logπ, grad_gaussian, γ, Ngrad, vec([0. 1.]), num_iter, constrain, κ, 1)\n",
    "    ψs_exponential = ADVI(logπ, grad_exponential, γ, Ngrad, 0.5, num_iter, constrain, κ, 2)\n",
    "\n",
    "    # compare objectives\n",
    "    obj_gaussian = est_obj(Nobj, ψs_gaussian[:,num_iter], 0., 1, logπ, ψs, λs, families, true)\n",
    "    push!(ψs, ψs_gaussian[:,num_iter])\n",
    "    families[1] = 1\n",
    "\n",
    "    println(\"component 1\")\n",
    "    println(families)\n",
    "    println(ψs)\n",
    "    \n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        λ_k = 1. / k\n",
    "        # optimize new component\n",
    "        λ_k_gau, ψ_k_gau = optimize_component(ψs, λs, λ_k, vec([0. 0.5]), logρ, γ, Ngrad, num_iter, families[1:k-1], 1, κ)\n",
    "        λs = (1. - λ_k_gau) .* λs\n",
    "        push!(λs, λ_k_gau)\n",
    "        push!(ψs, ψ_k_gau)\n",
    "        families[k] = 1\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"param: \", ψs[k])\n",
    "        println(\"weight: \", λs[k])\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return ψs, λs, families\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "mobile-abortion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "Int8[1, 0]\n",
      "Any[[1.4657170491878704, 1.667031311389476]]\n",
      "optimizing component 2 / 2\n",
      "component 2\n",
      "param: [-1.149992627941423, 0.33093973130258136]\n",
      "weight: 0.4854751263221256\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "logπ = x -> log(0.5 * pdf(Beta(1,6), x) + 0.5 * pdf(Beta(30,10), x))\n",
    "# logπ = x -> logpdf(Beta(50.,2.), x)\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 100\n",
    "Nobj = 100\n",
    "num_iter = 3000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "ψs, λs, families = VB_gaussian_only(logπ, γ, Ncomp, Ngrad, Nobj, num_iter, \n",
    "            grad_gaussian, grad_exponential, κ, constrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "raised-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_from_current_mixture(5000, ψs, λs, families)\n",
    "histogram(samples, normed=true,bins=50, alpha=0.3, label = \"approximation\")\n",
    "plot!(0:0.01:1, x -> exp(logπ(x)), label = \"target\", linewidth=1.5)\n",
    "png(\"gauss_multiple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
