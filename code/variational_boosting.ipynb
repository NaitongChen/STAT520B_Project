{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-north",
   "metadata": {},
   "source": [
    "# Extending Variational Boosting to Multiple Competing Variational Families\n",
    "\n",
    "This jupyter notebook contains all of the code used to generate the plots in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "fatty-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, ReverseDiff, ForwardDiff, SpecialFunctions, Distributions, Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-cable",
   "metadata": {},
   "source": [
    "## ADVI\n",
    "\n",
    "We begin by implementing the ADVI algorithm that can be used to obtain the first component in the mixture approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "incoming-iceland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADVI (generic function with 1 method)"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, num_iter)\n",
    "    Σs = zeros(d, d, num_iter)\n",
    "    obj = zeros(num_iter)\n",
    "\n",
    "    for k=1:num_iter\n",
    "        # transform ψ to mean and covariance\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        # store parameter\n",
    "        μs[:,k] = μ\n",
    "        Σs[:,:,k] = L * transpose(L)\n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        # compute the gradient\n",
    "        ∇ψ = grad(y, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "        # take a step\n",
    "        ψ0 = ψ0 .- γ(k) .* ∇ψ\n",
    "        # estimate the objective function\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        if d > 1\n",
    "            xobj = rand(MvNormal(μ, L * transpose(L)), Nobj)\n",
    "        else\n",
    "            xobj = rand(Normal(μ[1], L[1,1]^2), Nobj)\n",
    "            println(μ[1])\n",
    "            println(L[1,1]^2)\n",
    "        end\n",
    "        for i=1:size(xobj)[1]\n",
    "            obj[k] += logpdf(MvNormal(μ, L * transpose(L)), xobj[:,i]) - logπ(xobj[:,i])\n",
    "        end\n",
    "        obj[k] /= Nobj\n",
    "    end\n",
    "\n",
    "    return μs, Σs, obj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "guilty-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad (generic function with 1 method)"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transform(ψ, diag_sig, d)\n",
    "    μ = ψ[1:d]\n",
    "    if diag_sig\n",
    "        L = Diagonal(exp.(ψ[d+1:d+d]./2))\n",
    "    else\n",
    "        L = reshape(ψ[d+1:end], (d, d))\n",
    "    end\n",
    "    return μ, L\n",
    "end\n",
    "\n",
    "function logπψ(y, ψ0, logπ, diag_sig, d)\n",
    "    μ, L = transform(ψ0, diag_sig, d)\n",
    "    x = vec(μ .+ L * y)\n",
    "    return -(logπ(x) .+ log(abs(det(L))))\n",
    "end\n",
    "\n",
    "function grad(ys, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "    ret = zeros(size(ψ0)[1])\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])    \n",
    "        ret = ret .+ ReverseDiff.gradient(ψψ -> logπψ(y, ψψ, logπ, diag_sig, d), ψ0)\n",
    "    end\n",
    "    return (ret./Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-reserve",
   "metadata": {},
   "source": [
    "## Variational Boosting\n",
    "\n",
    "We now implement the variational boosting algorithm. First we implement the weighted EM method for initializing new components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "exciting-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_from_current_mixture (generic function with 1 method)"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function sample_from_current_mixture(μs, Σs, λs, Ngrad, d)\n",
    "    xs = zeros(d, Ngrad)\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:Ngrad\n",
    "        # identify component\n",
    "        u = rand(Uniform(0,1))\n",
    "        comp = 0\n",
    "        for j in 1:num_comp\n",
    "            if u <= sum(λs[1:j])\n",
    "                comp = j\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # sample from component\n",
    "        xs[:,i] = rand(MvNormal(vec(μs[:,comp]), Σs[:,:,comp]))\n",
    "    end\n",
    "    return xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "yellow-cancer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mixture (generic function with 2 methods)"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function log_mixture(μs, Σs, λs, x)\n",
    "    mix = 0.\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:num_comp\n",
    "        mix = mix + λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    return log(mix)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "failing-singer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_EM_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 943,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function compute_EM_weights(μs, Σs, λs, logπ, xs)\n",
    "    N = size(xs)[2]\n",
    "    logws = zeros(N)\n",
    "    for i in 1:N\n",
    "        x = xs[:,i]\n",
    "        logws[i] = logπ(x) - log_mixture(μs, Σs, λs, x)\n",
    "    end\n",
    "    return exp.(logws)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "split-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weighted_EM_initialization (generic function with 3 methods)"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function weighted_EM_initialization(μs, Σs, λs, weights, xs, diag_sig)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    N = size(xs)[2]\n",
    "    d = size(μs)[1]\n",
    "    λ = 1. / num_comp\n",
    "    μ = vec(zeros(d))\n",
    "    Σ = Diagonal(ones(d))\n",
    "    # EM loop\n",
    "    keep = true\n",
    "    while(keep)\n",
    "        # E step\n",
    "        resp = zeros(N, num_comp)\n",
    "        λs_curr = (1 - λ) .* λs\n",
    "        for i in 1:N\n",
    "            x = vec(xs[:,i])\n",
    "            for j in 1:num_comp\n",
    "                if j < num_comp\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μs[:,j]), Σs[:,:,j]), x) * λs_curr[j]\n",
    "                else\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μ),Σ), x) * λ\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        # normalize by dividing row sum\n",
    "        rowsum = sum(resp, dims=2)\n",
    "        resp = resp ./ rowsum\n",
    "        # M step\n",
    "        Nk = sum(resp, dims=1)[num_comp]\n",
    "        newλ = Nk / N\n",
    "        newμ = vec(zeros(d))\n",
    "        newΣ = zeros(d,d)\n",
    "        # update μ\n",
    "        for i in 1:N\n",
    "            newμ = newμ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* vec(xs[:,i])\n",
    "        end\n",
    "        # update Σ\n",
    "        for i in 1:N\n",
    "            diff = vec(xs[:,i]) .- newμ\n",
    "            newΣ = newΣ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* (diff) * transpose(diff)\n",
    "        end\n",
    "        # ensure Σ is Hermitian\n",
    "        for j in 1:d\n",
    "            for k in 1:j\n",
    "                newΣ[j,k] = newΣ[k,j]\n",
    "            end\n",
    "        end\n",
    "        # check convergence\n",
    "        if (newλ - λ)^2 <= 1e-3\n",
    "            println(\"finished initializing ψ and λ\")\n",
    "            keep = false\n",
    "        end\n",
    "        # update params\n",
    "        λ = newλ\n",
    "        μ = newμ\n",
    "        Σ = newΣ\n",
    "    end\n",
    "    # modify Σ to follow specification of diag_sig\n",
    "    if diag_sig\n",
    "        Σ = Diagonal(Σ)\n",
    "    end\n",
    "\n",
    "    return λ, μ, Σ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-paragraph",
   "metadata": {},
   "source": [
    "The following functions compute the gradient for component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "aware-glass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mix (generic function with 1 method)"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "    ret = 0.\n",
    "    for i in 1:size(μs)[2]\n",
    "        ret = ret + (1 - λ) * λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    ret = ret + λ * pdf(MvNormal(vec(μ_opt), L_opt * transpose(L_opt)), vec(x))\n",
    "    return log(ret)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "understood-sigma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function obj_comp(y, ψ, λ, logπ, diag_sig, d, μs, Σs, λs)\n",
    "    μ_opt, L_opt = transform(ψ, diag_sig, d)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    ret = 0.\n",
    "    for i in 1:num_comp\n",
    "        if i < num_comp\n",
    "            if diag_sig\n",
    "                L = Diagonal(sqrt.(Σs[:,:,i]))\n",
    "            else\n",
    "                C = cholesky(Σs[:,:,i])\n",
    "                L = convert(Array{Float64,2}, C.L)\n",
    "            end\n",
    "            x = vec(μs[:,i] .+ L * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - (1-λ) * λs[i] * logπ(x)\n",
    "            ret = ret + (1-λ) * λs[i] * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        else\n",
    "            x = vec(μ_opt .+ L_opt * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - λ * logπ(x)\n",
    "            ret = ret + λ * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "legislative-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function grad_comp(ys, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    retψ = zeros(size(ψ)[1])\n",
    "    retλ = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])\n",
    "        retψ = retψ .+ ForwardDiff.gradient(ψψ -> obj_comp(y, ψψ, λ, logπ, diag_sig, d, μs, Σs, λs), ψ)\n",
    "        retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logπ, diag_sig, d, μs, Σs, λs), λ)\n",
    "    end\n",
    "    return (retψ./Ngrad), (retλ/Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-integration",
   "metadata": {},
   "source": [
    "The following function performs the component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "supported-option",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize_component (generic function with 1 method)"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize_component(μs, Σs, λs, λ_k, μ_k, Σ_k, logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "    # turn μ_k and Σ_k to the format of ψ\n",
    "    ψ0 = copy(μ_k)\n",
    "    if diag_sig\n",
    "        append!(ψ0, log.([Σ_k[i,i] for i in 1:d]))\n",
    "    else\n",
    "        C = cholesky(Σ_k)\n",
    "        append!(ψ0, vec(C.L))\n",
    "    end\n",
    "    \n",
    "    ψ = vec(copy(ψ0))\n",
    "    println(ψ)\n",
    "    println(typeof(ψ))\n",
    "\n",
    "    # start optimization\n",
    "    λks = zeros(2*num_iter)\n",
    "    μks = zeros(d, num_iter)\n",
    "    Σks = zeros(d, d, num_iter)\n",
    "    λ = λ_k\n",
    "    \n",
    "    for k=1:(2*num_iter)\n",
    "        if k <= num_iter\n",
    "            # transform ψ to mean and covariance\n",
    "            μ, L = transform(ψ, diag_sig, d)\n",
    "            # store parameter\n",
    "            μks[:,k] = μ\n",
    "            Σks[:,:,k] = L * transpose(L)\n",
    "        end\n",
    "\n",
    "        λks[k] = λ\n",
    "        \n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        \n",
    "        # compute the gradient\n",
    "        ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "        # take a step\n",
    "        if k <= num_iter\n",
    "            ψ = ψ .- γ(k) .* ∇ψ\n",
    "        end\n",
    "        λ = λ - (γ(k)/200) * ∇λ\n",
    "        # project λ to feasible region\n",
    "        if λ < 0.\n",
    "            λ = 0.\n",
    "        elseif λ > 1.\n",
    "            λ = 1.\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"-----\")\n",
    "    y = rand(MvNormal(d, 1), Ngrad)\n",
    "    ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    println(\"ψ gradient: \", ∇ψ)\n",
    "    println(\"λ gradient: \", ∇λ)\n",
    "    println(\"-----\")\n",
    "    \n",
    "    return λks[2*num_iter], μks[:,num_iter], Σks[:,:,num_iter]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-allergy",
   "metadata": {},
   "source": [
    "The following function performs variational boosting, where the first component is optimized using ADVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "future-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 1 method)"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "\n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        xs = sample_from_current_mixture(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], 40, d)\n",
    "        weights = compute_EM_weights(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], logπ, xs)\n",
    "        λ_k, μ_k, Σ_k = weighted_EM_initialization(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], weights, xs, diag_sig)\n",
    "        println(\"initialization of weight, mean, and variance\")\n",
    "        println(λ_k)\n",
    "        println(μ_k)\n",
    "        println(Σ_k)\n",
    "        println(\"-----\")\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-reserve",
   "metadata": {},
   "source": [
    "We first demonstrate a case where the weighted EM algorithm fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ = vec([2. 2.])\n",
    "Σ = [1 0.; 0. 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -5:0.1:5\n",
    "x2s = -5:0.1:5\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "retained-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-attachment",
   "metadata": {},
   "source": [
    "We hence discard the weighted EM initialization and instead intialize the next component with a component centred at the origin with a large variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "driven-music",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 1 method)"
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "    \n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        λ_k = 1. / Ncomp\n",
    "        Σ_k = Diagonal(ones(d))\n",
    "        idx = Int64(floor(rand()*k))+1\n",
    "        μ_k = -vec(ones(d))\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "ceramic-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "mean: [2.9941526171455397, 2.9928674816328695]\n",
      "variance: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "-----\n",
      "optimizing component 2 / 2\n",
      "[-1.0, -1.0, 0.0, 0.0]\n",
      "Array{Float64,1}\n",
      "-----\n",
      "ψ gradient: [0.06557776016618942, 0.05759889421271219, 0.06281868025584292, 0.002957233010158445]\n",
      "λ gradient: -0.059942449696490326\n",
      "-----\n",
      "component 2\n",
      "mean: [-2.85627062883778, -2.8546278764693818]\n",
      "variance: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "weight: 0.4808757132944286\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "μ = vec([3. 3.])\n",
    "Σ = [1 0.1; 0.1 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "velvet-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [2.9941526171455397, 2.9928674816328695]\n",
      "sigma: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "lambda: 0.5191242867055714\n",
      "mu: [-2.85627062883778, -2.8546278764693818]\n",
      "sigma: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "lambda: 0.4808757132944286\n"
     ]
    }
   ],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "sudden-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -6:0.1:6\n",
    "x2s = -6:0.1:6\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "identical-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_close\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-passport",
   "metadata": {},
   "source": [
    "## Extending to Multiple Variational Families"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
