{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "small-motorcycle",
   "metadata": {},
   "source": [
    "# Extending Variational Boosting to Multiple Competing Variational Families\n",
    "\n",
    "This jupyter notebook contains all of the code used to generate the plots in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "divided-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, ReverseDiff, ForwardDiff, SpecialFunctions, Distributions, Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-sequence",
   "metadata": {},
   "source": [
    "## ADVI\n",
    "\n",
    "We begin by implementing the ADVI algorithm that can be used to obtain the first component in the mixture approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "numerical-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADVI (generic function with 1 method)"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, num_iter)\n",
    "    Σs = zeros(d, d, num_iter)\n",
    "    obj = zeros(num_iter)\n",
    "\n",
    "    for k=1:num_iter\n",
    "        # transform ψ to mean and covariance\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        # store parameter\n",
    "        μs[:,k] = μ\n",
    "        Σs[:,:,k] = L * transpose(L)\n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        # compute the gradient\n",
    "        ∇ψ = grad(y, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "        # take a step\n",
    "        ψ0 = ψ0 .- γ(k) .* ∇ψ\n",
    "        # estimate the objective function\n",
    "        μ, L = transform(ψ0, diag_sig, d)\n",
    "        xobj = rand(MvNormal(μ, L * transpose(L)), Nobj)\n",
    "        for i=1:size(xobj)[1]\n",
    "            obj[k] += logpdf(MvNormal(μ, L * transpose(L)), xobj[:,i]) - logπ(xobj[:,i])\n",
    "        end\n",
    "        obj[k] /= Nobj\n",
    "    end\n",
    "\n",
    "    return μs, Σs, obj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "expanded-truck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad (generic function with 1 method)"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transform(ψ, diag_sig, d)\n",
    "    μ = ψ[1:d]\n",
    "    if diag_sig\n",
    "        L = Diagonal(exp.(ψ[d+1:d+d]./2))\n",
    "    else\n",
    "        L = reshape(ψ[d+1:end], (d, d))\n",
    "    end\n",
    "    return μ, L\n",
    "end\n",
    "\n",
    "function logπψ(y, ψ0, logπ, diag_sig, d)\n",
    "    μ, L = transform(ψ0, diag_sig, d)\n",
    "    x = vec(μ .+ L * y)\n",
    "    return -(logπ(x) .+ log(abs(det(L))))\n",
    "end\n",
    "\n",
    "function grad(ys, ψ0, logπ, Ngrad, diag_sig, d)\n",
    "    ret = zeros(size(ψ0)[1])\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])    \n",
    "        ret = ret .+ ReverseDiff.gradient(ψψ -> logπψ(y, ψψ, logπ, diag_sig, d), ψ0)\n",
    "    end\n",
    "    return (ret./Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-transcription",
   "metadata": {},
   "source": [
    "## Variational Boosting\n",
    "\n",
    "We now implement the variational boosting algorithm. First we implement the weighted EM method for initializing new components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "parental-copying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_from_current_mixture (generic function with 1 method)"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function sample_from_current_mixture(μs, Σs, λs, Ngrad, d)\n",
    "    xs = zeros(d, Ngrad)\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:Ngrad\n",
    "        # identify component\n",
    "        u = rand(Uniform(0,1))\n",
    "        comp = 0\n",
    "        for j in 1:num_comp\n",
    "            if u <= sum(λs[1:j])\n",
    "                comp = j\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # sample from component\n",
    "        xs[:,i] = rand(MvNormal(vec(μs[:,comp]), Σs[:,:,comp]))\n",
    "    end\n",
    "    return xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "dental-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mixture (generic function with 2 methods)"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function log_mixture(μs, Σs, λs, x)\n",
    "    mix = 0.\n",
    "    num_comp = size(μs)[2]\n",
    "    for i in 1:num_comp\n",
    "        mix = mix + λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    return log(mix)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "multiple-alarm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_EM_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function compute_EM_weights(μs, Σs, λs, logπ, xs)\n",
    "    N = size(xs)[2]\n",
    "    logws = zeros(N)\n",
    "    for i in 1:N\n",
    "        x = xs[:,i]\n",
    "        logws[i] = logπ(x) - log_mixture(μs, Σs, λs, x)\n",
    "    end\n",
    "    \n",
    "    println(exp.(logws))\n",
    "    return exp.(logws)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "foster-dealer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weighted_EM_initialization (generic function with 3 methods)"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted EM helper\n",
    "function weighted_EM_initialization(μs, Σs, λs, weights, xs, diag_sig)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    N = size(xs)[2]\n",
    "    d = size(μs)[1]\n",
    "    λ = 1. / num_comp\n",
    "    μ = vec(zeros(d))\n",
    "    Σ = Diagonal(ones(d))\n",
    "    # EM loop\n",
    "    keep = true\n",
    "    while(keep)\n",
    "        # E step\n",
    "        resp = zeros(N, num_comp)\n",
    "        λs_curr = (1 - λ) .* λs\n",
    "        for i in 1:N\n",
    "            x = vec(xs[:,i])\n",
    "            for j in 1:num_comp\n",
    "                if j < num_comp\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μs[:,j]), Σs[:,:,j]), x) * λs_curr[j]\n",
    "                else\n",
    "                    resp[i,j] = pdf(MvNormal(vec(μ),Σ), x) * λ\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        # normalize by dividing row sum\n",
    "        rowsum = sum(resp, dims=2)\n",
    "        resp = resp ./ rowsum\n",
    "        # M step\n",
    "        Nk = sum(resp, dims=1)[num_comp]\n",
    "        newλ = Nk / N\n",
    "        newμ = vec(zeros(d))\n",
    "        newΣ = zeros(d,d)\n",
    "        # update μ\n",
    "        for i in 1:N\n",
    "            newμ = newμ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* vec(xs[:,i])\n",
    "        end\n",
    "        # update Σ\n",
    "        for i in 1:N\n",
    "            diff = vec(xs[:,i]) .- newμ\n",
    "            newΣ = newΣ .+ (1. / Nk) .* resp[i, num_comp] .* weights[i] .* (diff) * transpose(diff)\n",
    "        end\n",
    "        # ensure Σ is Hermitian\n",
    "        for j in 1:d\n",
    "            for k in 1:j\n",
    "                newΣ[j,k] = newΣ[k,j]\n",
    "            end\n",
    "        end\n",
    "        # check convergence\n",
    "        if (newλ - λ)^2 <= 1e-3\n",
    "            println(\"finished initializing ψ and λ\")\n",
    "            keep = false\n",
    "        end\n",
    "        # update params\n",
    "        λ = newλ\n",
    "        μ = newμ\n",
    "        Σ = newΣ\n",
    "    end\n",
    "    # modify Σ to follow specification of diag_sig\n",
    "    if diag_sig\n",
    "        Σ = Diagonal(Σ)\n",
    "    end\n",
    "\n",
    "    return λ, μ, Σ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-biography",
   "metadata": {},
   "source": [
    "The following functions compute the gradient for component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "wrapped-denmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_mix (generic function with 1 method)"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "    ret = 0.\n",
    "    for i in 1:size(μs)[2]\n",
    "        ret = ret + (1 - λ) * λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "    end\n",
    "    ret = ret + λ * pdf(MvNormal(vec(μ_opt), L_opt * transpose(L_opt)), vec(x))\n",
    "    return log(ret)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "twenty-above",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function obj_comp(y, ψ, λ, logπ, diag_sig, d, μs, Σs, λs)\n",
    "    μ_opt, L_opt = transform(ψ, diag_sig, d)\n",
    "    num_comp = size(μs)[2] + 1\n",
    "    ret = 0.\n",
    "    for i in 1:num_comp\n",
    "        if i < num_comp\n",
    "            if diag_sig\n",
    "                L = Diagonal(sqrt.(Σs[:,:,i]))\n",
    "            else\n",
    "                C = cholesky(Σs[:,:,i])\n",
    "                L = convert(Array{Float64,2}, C.L)\n",
    "            end\n",
    "            x = vec(μs[:,i] .+ L * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - (1-λ) * λs[i] * logπ(x)\n",
    "            ret = ret + (1-λ) * λs[i] * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        else\n",
    "            x = vec(μ_opt .+ L_opt * y)\n",
    "            # ln (mixture approx) - ln (π)\n",
    "            ret = ret - λ * logπ(x)\n",
    "            ret = ret + λ * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "paperback-columbus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_comp (generic function with 1 method)"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# component optimization gradient helper\n",
    "function grad_comp(ys, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    retψ = zeros(size(ψ)[1])\n",
    "    retλ = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = vec(ys[:,i])\n",
    "        retψ = retψ .+ ForwardDiff.gradient(ψψ -> obj_comp(y, ψψ, λ, logπ, diag_sig, d, μs, Σs, λs), ψ)\n",
    "        retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logπ, diag_sig, d, μs, Σs, λs), λ)\n",
    "    end\n",
    "    return (retψ./Ngrad), (retλ/Ngrad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-calgary",
   "metadata": {},
   "source": [
    "The following function performs the component optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "matched-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize_component (generic function with 1 method)"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize_component(μs, Σs, λs, λ_k, μ_k, Σ_k, logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "    # turn μ_k and Σ_k to the format of ψ\n",
    "    ψ0 = copy(μ_k)\n",
    "    if diag_sig\n",
    "        append!(ψ0, log.([Σ_k[i,i] for i in 1:d]))\n",
    "    else\n",
    "        C = cholesky(Σ_k)\n",
    "        append!(ψ0, vec(C.L))\n",
    "    end\n",
    "    \n",
    "    ψ = vec(copy(ψ0))\n",
    "    println(ψ)\n",
    "    println(typeof(ψ))\n",
    "\n",
    "    # start optimization\n",
    "    λks = zeros(2*num_iter)\n",
    "    μks = zeros(d, num_iter)\n",
    "    Σks = zeros(d, d, num_iter)\n",
    "    λ = λ_k\n",
    "    \n",
    "    for k=1:(2*num_iter)\n",
    "        if k <= num_iter\n",
    "            # transform ψ to mean and covariance\n",
    "            μ, L = transform(ψ, diag_sig, d)\n",
    "            # store parameter\n",
    "            μks[:,k] = μ\n",
    "            Σks[:,:,k] = L * transpose(L)\n",
    "        end\n",
    "\n",
    "        λks[k] = λ\n",
    "        \n",
    "        # sample from κ\n",
    "        y = rand(MvNormal(d, 1), Ngrad)\n",
    "        \n",
    "        # compute the gradient\n",
    "        ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "        # take a step\n",
    "        if k <= num_iter\n",
    "            ψ = ψ .- γ(k) .* ∇ψ\n",
    "        end\n",
    "        λ = λ - (γ(k)/200) * ∇λ\n",
    "        # project λ to feasible region\n",
    "        if λ < 0.\n",
    "            λ = 0.\n",
    "        elseif λ > 1.\n",
    "            λ = 1.\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"-----\")\n",
    "    y = rand(MvNormal(d, 1), Ngrad)\n",
    "    ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "    println(\"ψ gradient: \", ∇ψ)\n",
    "    println(\"λ gradient: \", ∇λ)\n",
    "    println(\"-----\")\n",
    "    \n",
    "    return λks[2*num_iter], μks[:,num_iter], Σks[:,:,num_iter]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-exclusion",
   "metadata": {},
   "source": [
    "The following function performs variational boosting, where the first component is optimized using ADVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "inside-apparel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 1 method)"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "\n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        xs = sample_from_current_mixture(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], 40, d)\n",
    "        weights = compute_EM_weights(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], logπ, xs)\n",
    "        λ_k, μ_k, Σ_k = weighted_EM_initialization(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], weights, xs, diag_sig)\n",
    "        println(\"initialization of weight, mean, and variance\")\n",
    "        println(λ_k)\n",
    "        println(μ_k)\n",
    "        println(Σ_k)\n",
    "        println(\"-----\")\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-agriculture",
   "metadata": {},
   "source": [
    "We first demonstrate a case where the weighted EM algorithm fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "clean-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "mean: [1.9707038202545093, 1.9694340881629466]\n",
      "variance: [1.0992736232511053 0.0; 0.0 1.1026546848108385]\n",
      "-----\n",
      "optimizing component 2 / 2\n",
      "[0.5327189451215595, 0.5017136131829105, 0.540461719305902, 0.48381084318745093, 0.5521877769390933, 0.5207485482723334, 0.5486813198544986, 0.5062282961260438, 0.41576286968420956, 0.49790262773233823, 0.47172198807627275, 0.5375937775272542, 0.4951387501887072, 0.5178332480708243, 0.5223591431477888, 0.5186392865899537, 0.5510055950817563, 0.49205109423039295, 0.4940926564614897, 0.47845911540806024, 0.5236116175830913, 0.4346887122483684, 0.4993523407260072, 0.4837488994332491, 0.5479484389430723, 0.4399662455292501, 0.5357114546778512, 0.5322096263902948, 0.5155967618334488, 0.4442263986538946, 0.394379519583017, 0.504200687695202, 0.5551649393906588, 0.5208367133116818, 0.5532140135104433, 0.5291721601183221, 0.486076318386388, 0.44590770269986657, 0.51257389028276, 0.4779403373593588]\n",
      "finished initializing ψ and λ\n",
      "initialization of weight, mean, and variance\n",
      "0.01457136058640468\n",
      "[0.3713336339259033, 0.3801856904582055]\n",
      "[0.1634726795059797 0.0; 0.0 0.13519870637750753]\n",
      "-----\n",
      "[0.3713336339259033, 0.3801856904582055, -1.8111094004299797, -2.0010096836327507]\n",
      "Array{Float64,1}\n",
      "-----\n",
      "ψ gradient: [0.002390915572379704, 0.0021498457015309306, -0.0005137436681598312, -0.0005066619918417735]\n",
      "λ gradient: 0.632482086953748\n",
      "-----\n",
      "component 2\n",
      "mean: [0.36434653135468625, 0.37154584859953504]\n",
      "variance: [0.16489604097628668 0.0; 0.0 0.13619115843002563]\n",
      "weight: 0.0016449270921134356\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "μ = vec([2. 2.])\n",
    "Σ = [1 0.; 0. 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "vocational-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [1.9707038202545093, 1.9694340881629466]\n",
      "sigma: [1.0992736232511053 0.0; 0.0 1.1026546848108385]\n",
      "lambda: 0.9983550729078866\n",
      "mu: [0.36434653135468625, 0.37154584859953504]\n",
      "sigma: [0.16489604097628668 0.0; 0.0 0.13619115843002563]\n",
      "lambda: 0.0016449270921134356\n"
     ]
    }
   ],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "general-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -5:0.1:5\n",
    "x2s = -5:0.1:5\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "id": "cubic-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-radical",
   "metadata": {},
   "source": [
    "We hence discard the weighted EM initialization and instead intialize the next component with a component centred at the origin with a large variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "established-linux",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VB (generic function with 1 method)"
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs = zeros(d, Ncomp)\n",
    "    Σs = zeros(d, d, Ncomp)\n",
    "    λs = [1.]\n",
    "    objs = zeros(Ncomp)\n",
    "\n",
    "    # fitting first component\n",
    "    println(\"optimizing component 1 / \", Ncomp)\n",
    "    mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "    μs[:,1] = mus[:,num_iter]\n",
    "    Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "    println(\"component 1\")\n",
    "    println(\"mean: \", μs[:,1])\n",
    "    println(\"variance: \", Σs[:,:,1])\n",
    "    println(\"-----\")\n",
    "    \n",
    "    # fitting remaining components\n",
    "    for k in 2:Ncomp\n",
    "        println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "        # init new component\n",
    "        λ_k = 1. / Ncomp\n",
    "        Σ_k = Diagonal(ones(d))\n",
    "        idx = Int64(floor(rand()*k))+1\n",
    "        μ_k = -vec(ones(d))\n",
    "\n",
    "        # optimize new component\n",
    "        λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "                                           logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "        # update parameters\n",
    "        λs = (1. - λ_k) .* λs\n",
    "        push!(λs, λ_k)\n",
    "        μs[:,k] .= μ_k\n",
    "        Σs[:,:,k] .= Σ_k\n",
    "\n",
    "        println(\"component \", k)\n",
    "        println(\"mean: \", μs[:,k])\n",
    "        println(\"variance: \", Σs[:,:,k])\n",
    "        println(\"weight: \", λ_k)\n",
    "        println(\"-----\")\n",
    "    end\n",
    "\n",
    "    return μs, Σs, λs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "spatial-brunswick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing component 1 / 2\n",
      "component 1\n",
      "mean: [2.9941526171455397, 2.9928674816328695]\n",
      "variance: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "-----\n",
      "optimizing component 2 / 2\n",
      "[-1.0, -1.0, 0.0, 0.0]\n",
      "Array{Float64,1}\n",
      "-----\n",
      "ψ gradient: [0.06557776016618942, 0.05759889421271219, 0.06281868025584292, 0.002957233010158445]\n",
      "λ gradient: -0.059942449696490326\n",
      "-----\n",
      "component 2\n",
      "mean: [-2.85627062883778, -2.8546278764693818]\n",
      "variance: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "weight: 0.4808757132944286\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "μ = vec([3. 3.])\n",
    "Σ = [1 0.1; 0.1 1]\n",
    "logπ = x -> log(0.5 * pdf(MvNormal(μ, Σ), x) + 0.5 * pdf(MvNormal(-μ, Σ), x))\n",
    "γ = k -> 0.1 /sqrt(k)\n",
    "Ngrad = 200\n",
    "Nobj = 200\n",
    "ψ0 = vec([1.5 1.5 1. 1.])\n",
    "diag_sig = true\n",
    "d = 2\n",
    "num_iter = 1000\n",
    "Ncomp = 2\n",
    "\n",
    "Random.seed!(1)\n",
    "μs, Σs, λs = VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "electrical-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: [2.9941526171455397, 2.9928674816328695]\n",
      "sigma: [1.0257877522063494 0.0; 0.0 1.0299760983657085]\n",
      "lambda: 0.5191242867055714\n",
      "mu: [-2.85627062883778, -2.8546278764693818]\n",
      "sigma: [1.016294174541848 0.0; 0.0 1.0195371494456904]\n",
      "lambda: 0.4808757132944286\n"
     ]
    }
   ],
   "source": [
    "for i in 1:Ncomp\n",
    "    println(\"mu: \", μs[:,i])\n",
    "    println(\"sigma: \", Σs[:,:,i])\n",
    "    println(\"lambda: \", λs[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "wireless-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "function πtrue(x,y)\n",
    "    return 0.5 * pdf(MvNormal(μ, Σ), vec([x y])) + 0.5 * pdf(MvNormal(-μ, Σ), vec([x y]))\n",
    "end\n",
    "\n",
    "function πapprox(x,y)\n",
    "    ret = 0.\n",
    "    for i in 1:Ncomp\n",
    "        ret = ret + λs[i] * pdf(MvNormal(μs[:,i], Σs[:,:,i]), vec([x y]))\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "x1s = -6:0.1:6\n",
    "x2s = -6:0.1:6\n",
    "\n",
    "contour(x1s, x2s, (x, y) -> πtrue(x,y), xlabel=\"x1\", ylabel=\"x2\", lw=1, levels=30)\n",
    "png(\"true_post_close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "continuing-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(x1s, x2s, (x, y) -> πapprox(x,y), lw=1, levels=30, xlabel=\"x1\", ylabel=\"x2\")\n",
    "png(\"approx_post_close\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-power",
   "metadata": {},
   "source": [
    "## Extending to Multiple Variational Families\n",
    "\n",
    "For this extension we focus on approximating mixture of Beta posteriors using Gaussian and exponential components. We first implement two transformations for Gaussian and exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "id": "irish-landscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "constrain_exponential (generic function with 1 method)"
      ]
     },
     "execution_count": 1094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes t in R and constrains it to be in (0,1)\n",
    "function constrain_gaussian(t)\n",
    "    lower = 0.\n",
    "    upper = 1.\n",
    "    x = lower + (upper-lower)/(1+exp(-t))\n",
    "    logJ = log(upper-lower)-t -2*(t > 0 ? log1p(exp(-t)) : (-t + log1p(exp(t))))\n",
    "    return x, logJ\n",
    "end\n",
    "\n",
    "# takes t in (0,∞) and constrains it to be in (0,1)\n",
    "function constrain_exponential(t)\n",
    "    x = 1/(t+1)\n",
    "    logJ = -2 * log(t+1)\n",
    "    return x, logJ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-chance",
   "metadata": {},
   "source": [
    "We now generalize the implementation of ADVI to both families of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "id": "corporate-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADVI (generic function with 3 methods)"
      ]
     },
     "execution_count": 1095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ADVI(logπ, grad, γ, Ngrad, ψ0, num_iter, constrain, κ, family)\n",
    "    # build the log density ρ that has unconstrained support\n",
    "    transform_logπ = (x, logJ) -> logπ(x) + logJ\n",
    "    logρ = x -> transform_logπ(constrain(x)...)\n",
    "    # store parameter values\n",
    "    if family == \"gaussian\"\n",
    "        ψs = zeros(2, num_iter)\n",
    "    elseif family == \"exponential\"\n",
    "        ψs = zeros(num_iter)\n",
    "    end\n",
    "    #main loop\n",
    "    for k=1:num_iter\n",
    "        # store parameters\n",
    "        if family == \"gaussian\"\n",
    "            ψs[:,k] = ψ0\n",
    "        elseif family == \"exponential\"\n",
    "            ψs[k] = ψ0\n",
    "        end\n",
    "        # sample from κ\n",
    "        ys = κ(Ngrad)\n",
    "        # compute the gradient\n",
    "        ∇ψ = grad(ys, ψ0, logρ, Ngrad)\n",
    "        # take a step\n",
    "        ψ0 = @. ψ0 - γ(k) * ∇ψ\n",
    "        if family == \"gaussian\" && ψ0[2] < 0\n",
    "            ψ0[2] = 0.\n",
    "        elseif family == \"exponential\" && ψ0 < 0\n",
    "            ψ0 = 0.\n",
    "        end\n",
    "    end\n",
    "    return ψs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-chicken",
   "metadata": {},
   "source": [
    "We now implement the functions that are specific to each variational family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "planned-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_exponential_q (generic function with 1 method)"
      ]
     },
     "execution_count": 1096,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## gaussian\n",
    "\n",
    "function κ_gaussian(Nobj)\n",
    "    return rand(Normal(), Nobj)\n",
    "end\n",
    "\n",
    "function logρψ_gaussian(y, ψ, logρ)\n",
    "    x = ψ[1] + ψ[2]*y\n",
    "    return -(logρ(x) + log(abs(ψ[2])))\n",
    "end\n",
    "\n",
    "function grad_gaussian(ys, ψ0, logρ, Ngrad)\n",
    "    ret = zeros(2)\n",
    "    for i in 1:Ngrad\n",
    "        y = ys[i]\n",
    "        ret = ret .+ ReverseDiff.gradient(ψψ -> logρψ_gaussian(y, ψψ, logρ), ψ0)\n",
    "    end\n",
    "    return (ret ./ Ngrad)\n",
    "end\n",
    "\n",
    "function sample_gaussian_q(N, ψ)\n",
    "    y = rand(Normal(0, 1), N)\n",
    "    t = ψ[1] .+ ψ[2] .* y\n",
    "    samps = zero(t)\n",
    "    for i=1:size(samps)[1]\n",
    "        samps[i] = constrain_gaussian(t[i])[1]\n",
    "    end\n",
    "    return samps\n",
    "end\n",
    "\n",
    "## exponential\n",
    "\n",
    "function κ_exponential(Nobj)\n",
    "    return rand(Uniform(), Nobj)\n",
    "end\n",
    "\n",
    "function logρψ_exponential(y, ψ, logρ)\n",
    "    x = -log(1 - y) / ψ\n",
    "    return -(logρ(x) + log(abs(ψ^-1 + 1/(1-y))))\n",
    "end\n",
    "\n",
    "function grad_exponential(ys, ψ0, logρ, Ngrad)\n",
    "    ret = 0.\n",
    "    for i in 1:Ngrad\n",
    "        y = ys[i]\n",
    "        ret = ret + ForwardDiff.derivative(ψψ -> logρψ_exponential(y, ψψ, logρ), ψ0)\n",
    "    end\n",
    "    return (ret / Ngrad)\n",
    "end\n",
    "\n",
    "function sample_exponential_q(N, ψ)\n",
    "    y = rand(Uniform(), N)\n",
    "    t = @. -log(1 - y) / ψ\n",
    "    samps = zero(t)\n",
    "    for i=1:size(samps)[1]\n",
    "        samps[i] = constrain_exponential(t[i])[1]\n",
    "    end\n",
    "    return samps\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-simple",
   "metadata": {},
   "source": [
    "We now implement Variational Boosting that considers both Gaussian and exponential components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # component optimization gradient helper\n",
    "# function log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "#     ret = 0.\n",
    "#     for i in 1:size(μs)[2]\n",
    "#         ret = ret + (1 - λ) * λs[i] * pdf(MvNormal(vec(μs[:,i]), Σs[:,:,i]), vec(x))\n",
    "#     end\n",
    "#     ret = ret + λ * pdf(MvNormal(vec(μ_opt), L_opt * transpose(L_opt)), vec(x))\n",
    "#     return log(ret)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # component optimization gradient helper\n",
    "# function obj_comp(y, ψ, λ, logπ, diag_sig, d, μs, Σs, λs)\n",
    "#     μ_opt, L_opt = transform(ψ, diag_sig, d)\n",
    "#     num_comp = size(μs)[2] + 1\n",
    "#     ret = 0.\n",
    "#     for i in 1:num_comp\n",
    "#         if i < num_comp\n",
    "#             if diag_sig\n",
    "#                 L = Diagonal(sqrt.(Σs[:,:,i]))\n",
    "#             else\n",
    "#                 C = cholesky(Σs[:,:,i])\n",
    "#                 L = convert(Array{Float64,2}, C.L)\n",
    "#             end\n",
    "#             x = vec(μs[:,i] .+ L * y)\n",
    "#             # ln (mixture approx) - ln (π)\n",
    "#             ret = ret - (1-λ) * λs[i] * logπ(x)\n",
    "#             ret = ret + (1-λ) * λs[i] * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "#         else\n",
    "#             x = vec(μ_opt .+ L_opt * y)\n",
    "#             # ln (mixture approx) - ln (π)\n",
    "#             ret = ret - λ * logπ(x)\n",
    "#             ret = ret + λ * log_mix(x, μs, Σs, λs, μ_opt, L_opt, λ)\n",
    "#         end\n",
    "#     end\n",
    "    \n",
    "#     return ret\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # component optimization gradient helper\n",
    "# function grad_comp(ys, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "#     retψ = zeros(size(ψ)[1])\n",
    "#     retλ = 0.\n",
    "#     for i in 1:Ngrad\n",
    "#         y = vec(ys[:,i])\n",
    "#         retψ = retψ .+ ForwardDiff.gradient(ψψ -> obj_comp(y, ψψ, λ, logπ, diag_sig, d, μs, Σs, λs), ψ)\n",
    "#         retλ = retλ + ForwardDiff.derivative(λλ -> obj_comp(y, ψ, λλ, logπ, diag_sig, d, μs, Σs, λs), λ)\n",
    "#     end\n",
    "#     return (retψ./Ngrad), (retλ/Ngrad)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function optimize_component(μs, Σs, λs, λ_k, μ_k, Σ_k, logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "#     # turn μ_k and Σ_k to the format of ψ\n",
    "#     ψ0 = copy(μ_k)\n",
    "#     if diag_sig\n",
    "#         append!(ψ0, log.([Σ_k[i,i] for i in 1:d]))\n",
    "#     else\n",
    "#         C = cholesky(Σ_k)\n",
    "#         append!(ψ0, vec(C.L))\n",
    "#     end\n",
    "    \n",
    "#     ψ = vec(copy(ψ0))\n",
    "#     println(ψ)\n",
    "#     println(typeof(ψ))\n",
    "\n",
    "#     # start optimization\n",
    "#     λks = zeros(2*num_iter)\n",
    "#     μks = zeros(d, num_iter)\n",
    "#     Σks = zeros(d, d, num_iter)\n",
    "#     λ = λ_k\n",
    "    \n",
    "#     for k=1:(2*num_iter)\n",
    "#         if k <= num_iter\n",
    "#             # transform ψ to mean and covariance\n",
    "#             μ, L = transform(ψ, diag_sig, d)\n",
    "#             # store parameter\n",
    "#             μks[:,k] = μ\n",
    "#             Σks[:,:,k] = L * transpose(L)\n",
    "#         end\n",
    "\n",
    "#         λks[k] = λ\n",
    "        \n",
    "#         # sample from κ\n",
    "#         y = rand(MvNormal(d, 1), Ngrad)\n",
    "        \n",
    "#         # compute the gradient\n",
    "#         ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "#         # take a step\n",
    "#         if k <= num_iter\n",
    "#             ψ = ψ .- γ(k) .* ∇ψ\n",
    "#         end\n",
    "#         λ = λ - (γ(k)/200) * ∇λ\n",
    "#         # project λ to feasible region\n",
    "#         if λ < 0.\n",
    "#             λ = 0.\n",
    "#         elseif λ > 1.\n",
    "#             λ = 1.\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "#     println(\"-----\")\n",
    "#     y = rand(MvNormal(d, 1), Ngrad)\n",
    "#     ∇ψ, ∇λ = grad_comp(y, ψ, λ, logπ, Ngrad, diag_sig, d, μs, Σs, λs)\n",
    "#     println(\"ψ gradient: \", ∇ψ)\n",
    "#     println(\"λ gradient: \", ∇λ)\n",
    "#     println(\"-----\")\n",
    "    \n",
    "#     return λks[2*num_iter], μks[:,num_iter], Σks[:,:,num_iter]\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function VB(logπ, grad, γ, Ncomp, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "#     μs = zeros(d, Ncomp)\n",
    "#     Σs = zeros(d, d, Ncomp)\n",
    "#     λs = [1.]\n",
    "#     objs = zeros(Ncomp)\n",
    "\n",
    "#     # fitting first component\n",
    "#     println(\"optimizing component 1 / \", Ncomp)\n",
    "#     mus, sigmas, obj = ADVI(logπ, grad, γ, Ngrad, Nobj, ψ0, d, num_iter, transform, diag_sig)\n",
    "#     μs[:,1] = mus[:,num_iter]\n",
    "#     Σs[:,:,1] = sigmas[:,:,num_iter]\n",
    "#     println(\"component 1\")\n",
    "#     println(\"mean: \", μs[:,1])\n",
    "#     println(\"variance: \", Σs[:,:,1])\n",
    "#     println(\"-----\")\n",
    "\n",
    "#     # fitting remaining components\n",
    "#     for k in 2:Ncomp\n",
    "#         println(\"optimizing component \", k, \" / \", Ncomp)\n",
    "#         # init new component\n",
    "#         xs = sample_from_current_mixture(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], 40, d)\n",
    "#         weights = compute_EM_weights(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], logπ, xs)\n",
    "#         λ_k, μ_k, Σ_k = weighted_EM_initialization(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], weights, xs, diag_sig)\n",
    "#         println(\"initialization of weight, mean, and variance\")\n",
    "#         println(λ_k)\n",
    "#         println(μ_k)\n",
    "#         println(Σ_k)\n",
    "#         println(\"-----\")\n",
    "\n",
    "#         # optimize new component\n",
    "#         λ_k, μ_k, Σ_k = optimize_component(μs[:,1:k-1], Σs[:,:,1:k-1], λs[1:k-1], λ_k, μ_k, Σ_k,\n",
    "#                                            logπ, γ, Ngrad, d, num_iter, transform, diag_sig)\n",
    "#         # update parameters\n",
    "#         λs = (1. - λ_k) .* λs\n",
    "#         push!(λs, λ_k)\n",
    "#         μs[:,k] .= μ_k\n",
    "#         Σs[:,:,k] .= Σ_k\n",
    "\n",
    "#         println(\"component \", k)\n",
    "#         println(\"mean: \", μs[:,k])\n",
    "#         println(\"variance: \", Σs[:,:,k])\n",
    "#         println(\"weight: \", λ_k)\n",
    "#         println(\"-----\")\n",
    "#     end\n",
    "\n",
    "#     return μs, Σs, λs\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
